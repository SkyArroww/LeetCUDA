# HardSharink

## 0x00 说明

包含以下内容：

- [X] hardshrink_f32_kernel
- [X] hardshrink_f32x4_kernel(float4向量化版本)
- [X] hardshrink_f16_kernel(fp16版本)
- [X] hardshrink_f16x2_kernel(fp16向量化版本)
- [X] hardshrink_f16x8_kernel(fp16向量化版本)
- [X] hardshrink_f16x8_pack_kernel(fp16向量化，pack版本)
- [X] PyTorch bindings


## 测试

```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada
python3 hardshrink.py
```

输出:

```bash
-------------------------------------------------------------------------------------
                                        S=1024, K=1024
           out_f32: ['0.0         ', '0.0         '], time:0.00428367ms
         out_f32x4: ['0.0         ', '0.0         '], time:0.00370097ms
        out_f32_th: ['0.0         ', '0.0         '], time:0.00739479ms
-------------------------------------------------------------------------------------
           out_f16: ['0.0         ', '0.0         '], time:0.00417209ms
         out_f16x2: ['0.0         ', '0.0         '], time:0.00280690ms
         out_f16x8: ['0.0         ', '0.0         '], time:0.00266337ms
     out_f16x8pack: ['0.0         ', '0.0         '], time:0.00255847ms
        out_f16_th: ['0.0         ', '0.0         '], time:0.00636697ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=2048
           out_f32: ['-0.82629085 ', '0.0         '], time:0.00588202ms
         out_f32x4: ['-0.82629085 ', '0.0         '], time:0.00565958ms
        out_f32_th: ['-0.82629085 ', '0.0         '], time:0.01137996ms
-------------------------------------------------------------------------------------
           out_f16: ['-0.82617188 ', '0.0         '], time:0.00589466ms
         out_f16x2: ['-0.82617188 ', '0.0         '], time:0.00484967ms
         out_f16x8: ['-0.82617188 ', '0.0         '], time:0.00404549ms
     out_f16x8pack: ['-0.82617188 ', '0.0         '], time:0.00391698ms
        out_f16_th: ['-0.82617188 ', '0.0         '], time:0.00740528ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=4096
           out_f32: ['0.0         ', '0.0         '], time:0.01001120ms
         out_f32x4: ['0.0         ', '0.0         '], time:0.00953746ms
        out_f32_th: ['0.0         ', '0.0         '], time:0.01943731ms
-------------------------------------------------------------------------------------
           out_f16: ['0.0         ', '0.0         '], time:0.01005745ms
         out_f16x2: ['0.0         ', '0.0         '], time:0.00977778ms
         out_f16x8: ['0.0         ', '0.0         '], time:0.00651360ms
     out_f16x8pack: ['0.0         ', '0.0         '], time:0.00592923ms
        out_f16_th: ['0.0         ', '0.0         '], time:0.01157641ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=1024
           out_f32: ['0.90714514  ', '0.0         '], time:0.00693989ms
         out_f32x4: ['0.90714514  ', '0.0         '], time:0.00567746ms
        out_f32_th: ['0.90714514  ', '0.0         '], time:0.01138806ms
-------------------------------------------------------------------------------------
           out_f16: ['0.90722656  ', '0.0         '], time:0.00694394ms
         out_f16x2: ['0.90722656  ', '0.0         '], time:0.00400567ms
         out_f16x8: ['0.90722656  ', '0.0         '], time:0.00397611ms
     out_f16x8pack: ['0.90722656  ', '0.0         '], time:0.00391245ms
        out_f16_th: ['0.90722656  ', '0.0         '], time:0.00740814ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=2048
           out_f32: ['-0.61146295 ', '-1.60048997 '], time:0.01001549ms
         out_f32x4: ['-0.61146295 ', '-1.60048997 '], time:0.00960755ms
        out_f32_th: ['-0.61146295 ', '-1.60048997 '], time:0.01942730ms
-------------------------------------------------------------------------------------
           out_f16: ['-0.61132812 ', '-1.60058594 '], time:0.01005864ms
         out_f16x2: ['-0.61132812 ', '-1.60058594 '], time:0.00810075ms
         out_f16x8: ['-0.61132812 ', '-1.60058594 '], time:0.00622988ms
     out_f16x8pack: ['-0.61132812 ', '-1.60058594 '], time:0.00591063ms
        out_f16_th: ['-0.61132812 ', '-1.60058594 '], time:0.01156282ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
           out_f32: ['0.0         ', '0.55151719  '], time:0.01828051ms
         out_f32x4: ['0.0         ', '0.55151719  '], time:0.01861715ms
        out_f32_th: ['0.0         ', '0.55151719  '], time:0.09329295ms
-------------------------------------------------------------------------------------
           out_f16: ['0.0         ', '0.55175781  '], time:0.01676941ms
         out_f16x2: ['0.0         ', '0.55175781  '], time:0.01659012ms
         out_f16x8: ['0.0         ', '0.55175781  '], time:0.00997281ms
     out_f16x8pack: ['0.0         ', '0.55175781  '], time:0.00901723ms
        out_f16_th: ['0.0         ', '0.55175781  '], time:0.01817441ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=1024
           out_f32: ['1.19508219  ', '0.75319326  '], time:0.01123738ms
         out_f32x4: ['1.19508219  ', '0.75319326  '], time:0.00885844ms
        out_f32_th: ['1.19508219  ', '0.75319326  '], time:0.01797009ms
-------------------------------------------------------------------------------------
           out_f16: ['1.1953125   ', '0.75341797  '], time:0.01144171ms
         out_f16x2: ['1.1953125   ', '0.75341797  '], time:0.00580645ms
         out_f16x8: ['1.1953125   ', '0.75341797  '], time:0.00570226ms
     out_f16x8pack: ['1.1953125   ', '0.75341797  '], time:0.00586438ms
        out_f16_th: ['1.1953125   ', '0.75341797  '], time:0.01156831ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=2048
           out_f32: ['0.58269709  ', '-0.82688355 '], time:0.01827741ms
         out_f32x4: ['0.58269709  ', '-0.82688355 '], time:0.01818061ms
        out_f32_th: ['0.58269709  ', '-0.82688355 '], time:0.09307647ms
-------------------------------------------------------------------------------------
           out_f16: ['0.58251953  ', '-0.82666016 '], time:0.01676655ms
         out_f16x2: ['0.58251953  ', '-0.82666016 '], time:0.01336956ms
         out_f16x8: ['0.58251953  ', '-0.82666016 '], time:0.00968409ms
     out_f16x8pack: ['0.58251953  ', '-0.82666016 '], time:0.00905466ms
        out_f16_th: ['0.58251953  ', '-0.82666016 '], time:0.01817107ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=4096
           out_f32: ['0.0         ', '0.0         '], time:0.14530301ms
         out_f32x4: ['0.0         ', '0.0         '], time:0.14572287ms
        out_f32_th: ['0.0         ', '0.0         '], time:0.29304075ms
-------------------------------------------------------------------------------------
           out_f16: ['0.0         ', '0.0         '], time:0.03192568ms
         out_f16x2: ['0.0         ', '0.0         '], time:0.03173161ms
         out_f16x8: ['0.0         ', '0.0         '], time:0.01897788ms
     out_f16x8pack: ['0.0         ', '0.0         '], time:0.01702213ms
        out_f16_th: ['0.0         ', '0.0         '], time:0.09349132ms
-------------------------------------------------------------------------------------

```

## 0x01 Kernel分析

### HardShrink函数原理
HardShrink是一个激活函数，数学公式为：
```
f(x) = x, if |x| > λ
f(x) = 0, otherwise
```
其中λ为阈值参数（代码中设为0.5）。该函数将绝对值小于阈值的值置零，保留绝对值大于阈值的值不变。

### Kernel实现分析

#### 1. hardshrink_f32_kernel
**原理：** 基础的单元素处理kernel，每个线程处理一个float32元素。
**优化：** 
- 简单直接的实现，适合理解基本逻辑
- 内存访问模式：每线程1个float（4字节）

#### 2. hardshrink_f32x4_kernel  
**原理：** 向量化处理，每个线程处理4个float32元素。
**优化：**
- 使用float4向量化内存访问，提高内存带宽利用率
- 单次内存事务处理16字节数据
- 减少内存访问次数，提高缓存效率

#### 3. hardshrink_f16_kernel
**原理：** 单个half精度元素处理。
**优化：**
- 使用FP16数据类型，减少内存占用和带宽需求
- 利用现代GPU的FP16计算单元加速

#### 4. hardshrink_f16x2_kernel
**原理：** 使用half2向量化处理2个FP16元素。
**优化：**
- half2打包处理，单次访问4字节处理2个元素
- 利用GPU的SIMD指令集
- 相比单元素版本减少了线程调度开销

#### 5. hardshrink_f16x8_kernel
**原理：** 每线程处理8个FP16元素，使用4个half2。
**优化：**
- 更高级别的向量化，单线程处理16字节数据
- 更好的内存合并访问模式
- 减少kernel启动开销和线程数量
- 包含边界检查以处理不对齐的数据

#### 6. hardshrink_f16x8_pack_kernel
**原理：** 最优化版本，使用128位内存事务处理8个FP16元素。
**优化：**
- 使用LDST128BITS宏实现128位对齐内存访问
- 最大化内存带宽利用率（单次访问16字节）
- pack/unpack操作在寄存器中进行，避免额外内存访问
- 最佳的内存合并访问模式

### 优化技术总结

#### 1. 向量化优化
- **Float4向量化：** 将4个float32打包处理，提高内存带宽利用率
- **Half2向量化：** 利用GPU原生half2支持，单指令处理双元素
- **更高阶向量化：** f16x8版本进一步提高并行度

#### 2. 内存访问优化  
- **内存对齐：** 使用128位对齐访问（LDST128BITS）最大化带宽
- **内存合并：** 连续线程访问连续内存地址，提高缓存命中率
- **减少内存事务：** 通过向量化减少总的内存访问次数

#### 3. 数据类型优化
- **FP16优势：** 相比FP32减少一半内存占用和带宽需求
- **精度权衡：** 对于激活函数场景，FP16精度通常足够
- **硬件加速：** 现代GPU对FP16有专门的计算单元支持

#### 4. 线程和块优化
- **自适应网格配置：** 根据数据形状选择最优的grid/block配置  
- **占用率优化：** 通过合理的block大小最大化SM占用率
- **边界处理：** 在向量化版本中正确处理数据边界

### 性能对比分析
从测试结果可以看出：
1. **FP16版本普遍快于FP32版本**：内存带宽优势明显
2. **向量化效果显著**：f16x8_pack > f16x8 > f16x2 > f16
3. **PyTorch baseline相对较慢**：说明自定义kernel的优化价值
4. **大数据量时优势更明显**：向量化和内存优化在大规模数据时效果更佳

### 适用场景
- **训练阶段：** 可用于神经网络的正则化和稀疏化
- **推理优化：** 特别适合需要快速激活函数计算的场景  
- **大规模计算：** 向量化版本适合处理大型张量运算
