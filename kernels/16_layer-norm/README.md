# LayerNorm

## 0x00 说明

包含以下内容：

- [X] layer_norm_f32_kernel
- [X] layer_norm_f32x4_kernel
- [X] layer_norm_f16_f16_kernel
- [X] layer_norm_f16x2_f16_kernel
- [X] layer_norm_f16x8_f16_kernel
- [X] layer_norm_f16x8_pack_f16_kernel
- [X] layer_norm_f16x8_pack_f32_kernel
- [X] layer_norm_f16_f32_kernel
- [X] PyTorch bindings

## 测试

```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada
python3 layer_norm.py
```

输出:

```bash
-------------------------------------------------------------------------------------
                                        N=4096, K=512
-------------------------------------------------------------------------------------
          out_f32: ['-0.95119929 ', '0.65728813  ', '-0.27701864 '], time:0.01898599ms
        out_f32x4: ['-0.95119929 ', '0.65728813  ', '-0.27701864 '], time:0.00600958ms
       out_f32_th: ['-0.95026982 ', '0.65664589  ', '-0.27674797 '], time:0.07345414ms
-------------------------------------------------------------------------------------
       out_f16f16: ['-0.95068359 ', '0.65722656  ', '-0.27709961 '], time:0.01866651ms
       out_f16f32: ['-0.95117188 ', '0.65722656  ', '-0.27709961 '], time:0.01897073ms
     out_f16x2f16: ['-0.95068359 ', '0.65722656  ', '-0.27709961 '], time:0.00952697ms
     out_f16x8f16: ['-0.95068359 ', '0.65722656  ', '-0.27709961 '], time:0.00470805ms
 out_f16x8packf16: ['-0.95117188 ', '0.65673828  ', '-0.27709961 '], time:0.00427437ms
 out_f16x8packf32: ['-0.95117188 ', '0.65722656  ', '-0.27709961 '], time:0.00418639ms
       out_f16_th: ['-0.94970703 ', '0.65673828  ', '-0.27685547 '], time:0.07291913ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        N=4096, K=1024
-------------------------------------------------------------------------------------
          out_f32: ['0.81839228  ', '0.36616057  ', '-1.71588480 '], time:0.05122757ms
        out_f32x4: ['0.81839228  ', '0.36616057  ', '-1.71588480 '], time:0.01071095ms
       out_f32_th: ['0.81799269  ', '0.36598179  ', '-1.71504688 '], time:0.07267237ms
-------------------------------------------------------------------------------------
       out_f16f16: ['0.81835938  ', '0.36596680  ', '-1.71484375 '], time:0.05317926ms
       out_f16f32: ['0.81835938  ', '0.36621094  ', '-1.71582031 '], time:0.05062103ms
     out_f16x2f16: ['0.81884766  ', '0.36621094  ', '-1.71679688 '], time:0.01855445ms
     out_f16x8f16: ['0.81884766  ', '0.36621094  ', '-1.71679688 '], time:0.00742888ms
 out_f16x8packf16: ['0.81884766  ', '0.36621094  ', '-1.71679688 '], time:0.00645399ms
 out_f16x8packf32: ['0.81835938  ', '0.36621094  ', '-1.71582031 '], time:0.00634456ms
       out_f16_th: ['0.81835938  ', '0.36596680  ', '-1.71582031 '], time:0.07386255ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        N=4096, K=2048
-------------------------------------------------------------------------------------
        out_f32x4: ['-0.65341073 ', '0.10270299  ', '-0.06597849 '], time:0.02200651ms
       out_f32_th: ['-0.65325129 ', '0.10267793  ', '-0.06596238 '], time:0.12027287ms
-------------------------------------------------------------------------------------
     out_f16x2f16: ['-0.65332031 ', '0.10266113  ', '-0.06591797 '], time:0.05352354ms
     out_f16x8f16: ['-0.65380859 ', '0.10272217  ', '-0.06597900 '], time:0.01377678ms
 out_f16x8packf16: ['-0.65332031 ', '0.10266113  ', '-0.06591797 '], time:0.01154637ms
 out_f16x8packf32: ['-0.65332031 ', '0.10272217  ', '-0.06597900 '], time:0.01166582ms
       out_f16_th: ['-0.65380859 ', '0.10272217  ', '-0.06597900 '], time:0.08442783ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        N=4096, K=4096
-------------------------------------------------------------------------------------
        out_f32x4: ['2.38733387  ', '-0.03023042 ', '0.66022825  '], time:0.18884635ms
       out_f32_th: ['2.38704205  ', '-0.03022672 ', '0.66014749  '], time:0.77852798ms
-------------------------------------------------------------------------------------
     out_f16x8f16: ['2.38671875  ', '-0.03024292 ', '0.66015625  '], time:0.03325391ms
 out_f16x8packf16: ['2.38671875  ', '-0.03024292 ', '0.66015625  '], time:0.02401376ms
 out_f16x8packf32: ['2.38671875  ', '-0.03021240 ', '0.66064453  '], time:0.02381730ms
       out_f16_th: ['2.38671875  ', '-0.03021240 ', '0.66015625  '], time:0.17546010ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        N=4096, K=8192
-------------------------------------------------------------------------------------
     out_f16x8f16: ['0.15905762  ', '1.06542969  ', '-0.19396973 '], time:0.19306803ms
 out_f16x8packf16: ['0.15905762  ', '1.06542969  ', '-0.19396973 '], time:0.18665886ms
 out_f16x8packf32: ['0.15905762  ', '1.06542969  ', '-0.19396973 '], time:0.18657684ms
       out_f16_th: ['0.15905762  ', '1.06542969  ', '-0.19396973 '], time:0.84462571ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        N=8192, K=8192
-------------------------------------------------------------------------------------
     out_f16x8f16: ['-0.53662109 ', '2.359375    ', '0.78027344  '], time:0.38366604ms
 out_f16x8packf16: ['-0.53662109 ', '2.359375    ', '0.78027344  '], time:0.40789628ms
 out_f16x8packf32: ['-0.53613281 ', '2.359375    ', '0.78027344  '], time:0.40818143ms
       out_f16_th: ['-0.53662109 ', '2.359375    ', '0.78027344  '], time:1.99523735ms
-------------------------------------------------------------------------------------
```

## 0x01 Layer Normalization 工作原理

Layer Normalization是深度学习中常用的归一化技术，其数学公式为：

```
mean = sum(x_i) / K
variance = sum((x_i - mean)^2) / K  
y_i = (x_i - mean) / sqrt(variance + epsilon) * gamma + beta
```

其中：
- `x_i` 是输入的第i个元素
- `K` 是特征维度大小（hidden_size）
- `epsilon` 是防止除零的小常数（通常为1e-5）
- `gamma` 和 `beta` 是可学习的缩放和偏移参数

## 0x02 Kernel实现原理与优化分析

### 1. 基础版本 - `layer_norm_f32_kernel`

**实现原理：**
- 使用FP32精度进行所有计算
- 每个线程处理一个元素
- 采用两阶段归约：先在warp内归约，再在block内归约

**优化策略：**
- **Warp Shuffle归约**: 使用`__shfl_xor_sync`进行高效的warp内归约
- **共享内存优化**: 只使用少量共享内存存储每个warp的归约结果
- **双阶段计算**: 分别计算均值和方差，避免数值不稳定

**性能特点：**
- 计算精度高，数值稳定
- 内存带宽利用率一般（每线程1个元素）

### 2. 向量化版本 - `layer_norm_f32x4_kernel`

**实现原理：**
- 每个线程处理4个FP32元素，使用`float4`向量化加载
- 线程数减少为K/4，提高内存带宽利用率

**优化策略：**
- **内存合并访问**: 使用`FLOAT4`宏进行128位对齐加载
- **向量化计算**: 手动展开4个元素的计算
- **减少线程发散**: 更少的线程数减少了warp内的发散

**性能提升：**
- 相比基础版本有3-4倍性能提升
- 内存带宽利用率提高4倍

### 3. 混合精度版本 - `layer_norm_f16_f16_kernel`

**实现原理：**
- 输入输出都使用FP16，中间计算也使用FP16
- 专门实现了FP16的warp归约函数

**优化策略：**
- **FP16原生运算**: 使用`__hadd`, `__hsub`, `__hmul`, `hrsqrt`等FP16指令
- **FP16归约**: 实现专门的`warp_reduce_sum_f16_f16`函数
- **FMA指令**: 使用`__hfma`进行融合乘加运算

**性能特点：**
- 内存占用减半
- 计算速度提升（FP16 Tensor Core支持）
- 精度略有损失

### 4. 混合精度计算版本 - `layer_norm_f16_f32_kernel`

**实现原理：**
- 输入输出使用FP16，中间计算使用FP32
- 结合了FP16的内存优势和FP32的计算精度

**优化策略：**
- **精度权衡**: 关键计算步骤使用FP32保证数值稳定性
- **类型转换优化**: 使用`__half2float`和`__float2half`进行高效转换
- **FMA优化**: 使用`__fmaf_rn`进行融合乘加

### 5. FP16向量化版本系列

#### `layer_norm_f16x2_f16_kernel`
- 每线程处理2个FP16元素，使用`half2`向量
- 使用宏定义简化重复代码
- 2倍内存带宽提升

#### `layer_norm_f16x8_f16_kernel` 
- 每线程处理8个FP16元素
- 手动展开循环，最大化向量化
- 使用宏定义`HALF2_SUM`, `HALF2_VARIANCE`等简化代码

#### `layer_norm_f16x8_pack_f16_kernel`
- **最高优化版本**，使用128位打包加载
- 临时数组存储在寄存器中（.local space）
- 单次内存事务加载128位数据

**核心优化：**
```cuda
half pack_x[8], pack_y[8]; // 寄存器数组
LDST128BITS(pack_x[0]) = LDST128BITS(x[idx]); // 128位加载
```

#### `layer_norm_f16x8_pack_f32_kernel`
- 结合128位打包加载和FP32精度计算
- 在精度和性能之间达到最佳平衡

## 0x03 优化技术总结

### 1. 内存访问优化
| 技术 | 描述 | 性能提升 |
|------|------|----------|
| 向量化加载 | 使用float4/half2批量加载 | 2-4倍 |
| 128位对齐 | LDST128BITS宏实现最宽内存访问 | 最大化带宽 |
| 内存合并 | 确保连续线程访问连续内存 | 避免缓存miss |

### 2. 计算优化
| 技术 | 描述 | 效果 |
|------|------|------|
| Warp Shuffle | `__shfl_xor_sync`实现高效归约 | 避免共享内存 |
| 融合乘加 | `__fmaf_rn`, `__hfma`指令 | 减少指令数 |
| 循环展开 | 手动展开向量化计算 | 减少分支开销 |

### 3. 精度策略
| 版本 | 输入 | 计算 | 输出 | 适用场景 |
|------|------|------|------|----------|
| F32 | FP32 | FP32 | FP32 | 训练，需要高精度 |
| F16F16 | FP16 | FP16 | FP16 | 推理，追求极致性能 |
| F16F32 | FP16 | FP32 | FP16 | 平衡精度与性能 |
| F16x8Pack | FP16 | FP16/32 | FP16 | 生产环境最优选择 |

### 4. 性能分析

从测试结果可以看出：

**FP32版本：**
- `layer_norm_f32`: 基准性能
- `layer_norm_f32x4`: 3-4倍性能提升

**FP16版本性能排序：**
1. `layer_norm_f16x8_pack_f32`: **最佳性能** (~0.006ms)
2. `layer_norm_f16x8_pack_f16`: 接近最佳 (~0.006ms) 
3. `layer_norm_f16x8_f16`: 8倍向量化 (~0.007ms)
4. `layer_norm_f16x2_f16`: 2倍向量化 (~0.019ms)
5. `layer_norm_f16_f16/f32`: 基础版本 (~0.050ms)

**关键结论：**
- 向量化是最重要的优化手段
- 128位对齐加载带来显著性能提升
- FP16在保持精度的同时大幅提升性能
- Pack版本通过寄存器数组实现了最优的内存访问模式

## 0x04 适用场景建议

- **训练阶段**: 使用`layer_norm_f32x4`保证数值稳定性
- **推理阶段**: 使用`layer_norm_f16x8_pack_f32`获得最佳性能
- **内存受限**: 使用`layer_norm_f16x8_pack_f16`最小化内存占用
- **精度要求高**: 使用`layer_norm_f16_f32`平衡精度与性能
