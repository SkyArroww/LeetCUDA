# embedding

## 0x00 说明

包含以下内容：

- [X] embedding_f32_kernel
- [X] embedding_f32x4_kernel(float4向量化版本)
- [X] embedding_f32x4_pack_kernel(float4向量化，pack版本版本)
- [X] embedding_f16_kernel(fp16版本)
- [X] embedding_f16x8_kernel(fp16向量化版本)
- [X] embedding_f16x8_pack_kernel(fp16向量化，pack版本)
- [X] PyTorch bindings


## 测试

```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada
python3 embedding.py
```
一个elemwise的操作，但是有个值得探究的问题，在f16下pack的性能优于没有pack的性能但是在f32下相反
输出:

```bash
--------------------------------------------------------------------------------------------------------------
                                             MaxV=1024, SeqLen=2048, EmbSize=512
                out_f32: ['0.69075936  ', '0.56517494  ', '-0.12546943 '], time:0.005317ms
              out_f32x4: ['0.69075936  ', '0.56517494  ', '-0.12546943 '], time:0.004125ms
         out_f32x4_pack: ['0.69075936  ', '0.56517494  ', '-0.12546943 '], time:0.004017ms
             out_f32_th: ['0.69075936  ', '0.56517494  ', '-0.12546943 '], time:0.012147ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['-1.27734375 ', '-0.92822266 ', '-1.4453125  '], time:0.005090ms
              out_f16x8: ['-1.27734375 ', '-0.92822266 ', '-1.4453125  '], time:0.004089ms
         out_f16x8_pack: ['-1.27734375 ', '-0.92822266 ', '-1.4453125  '], time:0.004041ms
             out_f16_th: ['-1.27734375 ', '-0.92822266 ', '-1.4453125  '], time:0.011230ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=1024, SeqLen=2048, EmbSize=1024
                out_f32: ['-1.34922504 ', '-0.04674992 ', '1.24448562  '], time:0.011468ms
              out_f32x4: ['-1.34922504 ', '-0.04674992 ', '1.24448562  '], time:0.005364ms
         out_f32x4_pack: ['-1.34922504 ', '-0.04674992 ', '1.24448562  '], time:0.005448ms
             out_f32_th: ['-1.34922504 ', '-0.04674992 ', '1.24448562  '], time:0.037062ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['-0.47412109 ', '-0.47070312 ', '1.41894531  '], time:0.011039ms
              out_f16x8: ['-0.47412109 ', '-0.47070312 ', '1.41894531  '], time:0.004971ms
         out_f16x8_pack: ['-0.47412109 ', '-0.47070312 ', '1.41894531  '], time:0.004065ms
             out_f16_th: ['-0.47412109 ', '-0.47070312 ', '1.41894531  '], time:0.016463ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=1024, SeqLen=4096, EmbSize=512
                out_f32: ['0.30875409  ', '0.98055625  ', '-0.86661887 '], time:0.008464ms
              out_f32x4: ['0.30875409  ', '0.98055625  ', '-0.86661887 '], time:0.005400ms
         out_f32x4_pack: ['0.30875409  ', '0.98055625  ', '-0.86661887 '], time:0.005484ms
             out_f32_th: ['0.30875409  ', '0.98055625  ', '-0.86661887 '], time:0.016463ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['-1.04492188 ', '0.44750977  ', '-1.25878906 '], time:0.008070ms
              out_f16x8: ['-1.04492188 ', '0.44750977  ', '-1.25878906 '], time:0.005186ms
         out_f16x8_pack: ['-1.04492188 ', '0.44750977  ', '-1.25878906 '], time:0.004339ms
             out_f16_th: ['-1.04492188 ', '0.44750977  ', '-1.25878906 '], time:0.016415ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=1024, SeqLen=4096, EmbSize=1024
                out_f32: ['-0.07979927 ', '-0.20634571 ', '0.9166832   '], time:0.021017ms
              out_f32x4: ['-0.07979927 ', '-0.20634571 ', '0.9166832   '], time:0.008297ms
         out_f32x4_pack: ['-0.07979927 ', '-0.20634571 ', '0.9166832   '], time:0.008714ms
             out_f32_th: ['-0.07979927 ', '-0.20634571 ', '0.9166832   '], time:0.037730ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['-2.19726562 ', '0.50439453  ', '1.40917969  '], time:0.020158ms
              out_f16x8: ['-2.19726562 ', '0.50439453  ', '1.40917969  '], time:0.007451ms
         out_f16x8_pack: ['-2.19726562 ', '0.50439453  ', '1.40917969  '], time:0.005496ms
             out_f16_th: ['-2.19726562 ', '0.50439453  ', '1.40917969  '], time:0.030172ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=4096, SeqLen=2048, EmbSize=512
                out_f32: ['-0.13596091 ', '-0.07996719 ', '-2.77454519 '], time:0.005329ms
              out_f32x4: ['-0.13596091 ', '-0.07996719 ', '-2.77454519 '], time:0.004041ms
         out_f32x4_pack: ['-0.13596091 ', '-0.07996719 ', '-2.77454519 '], time:0.004005ms
             out_f32_th: ['-0.13596091 ', '-0.07996719 ', '-2.77454519 '], time:0.011110ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['0.14440918  ', '0.26367188  ', '0.77539062  '], time:0.005066ms
              out_f16x8: ['0.14440918  ', '0.26367188  ', '0.77539062  '], time:0.004041ms
         out_f16x8_pack: ['0.14440918  ', '0.26367188  ', '0.77539062  '], time:0.003982ms
             out_f16_th: ['0.14440918  ', '0.26367188  ', '0.77539062  '], time:0.011170ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=4096, SeqLen=2048, EmbSize=1024
                out_f32: ['1.26920044  ', '0.12124556  ', '0.38764721  '], time:0.011575ms
              out_f32x4: ['1.26920044  ', '0.12124556  ', '0.38764721  '], time:0.005329ms
         out_f32x4_pack: ['1.26920044  ', '0.12124556  ', '0.38764721  '], time:0.005519ms
             out_f32_th: ['1.26920044  ', '0.12124556  ', '0.38764721  '], time:0.016499ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['0.49243164  ', '-0.44116211 ', '-0.14611816 '], time:0.011182ms
              out_f16x8: ['0.49243164  ', '-0.44116211 ', '-0.14611816 '], time:0.004947ms
         out_f16x8_pack: ['0.49243164  ', '-0.44116211 ', '-0.14611816 '], time:0.004029ms
             out_f16_th: ['0.49243164  ', '-0.44116211 ', '-0.14611816 '], time:0.016606ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=4096, SeqLen=4096, EmbSize=512
                out_f32: ['-1.29970169 ', '-1.45127702 ', '-0.7259807  '], time:0.008452ms
              out_f32x4: ['-1.29970169 ', '-1.45127702 ', '-0.7259807  '], time:0.005424ms
         out_f32x4_pack: ['-1.29970169 ', '-1.45127702 ', '-0.7259807  '], time:0.005460ms
             out_f32_th: ['-1.29970169 ', '-1.45127702 ', '-0.7259807  '], time:0.016463ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['0.88623047  ', '0.00491714  ', '0.38525391  '], time:0.008094ms
              out_f16x8: ['0.88623047  ', '0.00491714  ', '0.38525391  '], time:0.005186ms
         out_f16x8_pack: ['0.88623047  ', '0.00491714  ', '0.38525391  '], time:0.004339ms
             out_f16_th: ['0.88623047  ', '0.00491714  ', '0.38525391  '], time:0.016451ms
--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
                                             MaxV=4096, SeqLen=4096, EmbSize=1024
                out_f32: ['0.62899369  ', '0.55146962  ', '-0.03596229 '], time:0.021005ms
              out_f32x4: ['0.62899369  ', '0.55146962  ', '-0.03596229 '], time:0.008404ms
         out_f32x4_pack: ['0.62899369  ', '0.55146962  ', '-0.03596229 '], time:0.008738ms
             out_f32_th: ['0.62899369  ', '0.55146962  ', '-0.03596229 '], time:0.056100ms
--------------------------------------------------------------------------------------------------------------
                out_f16: ['1.13085938  ', '1.19628906  ', '-0.61035156 '], time:0.020254ms
              out_f16x8: ['1.13085938  ', '1.19628906  ', '-0.61035156 '], time:0.007451ms
         out_f16x8_pack: ['1.13085938  ', '1.19628906  ', '-0.61035156 '], time:0.005472ms
             out_f16_th: ['1.13085938  ', '1.19628906  ', '-0.61035156 '], time:0.030160ms
--------------------------------------------------------------------------------------------------------------
```

## 0x01 Kernel原理与优化分析

### Embedding操作基本原理

Embedding操作是将离散的token ID映射到连续向量表示的基础操作。其核心是根据输入索引从embedding权重矩阵中查找对应的向量：

```
对于输入索引 idx[i]，输出: output[i, :] = weight[idx[i], :]
```

### 各Kernel详细分析

#### 1. embedding_f32_kernel (基础版本)
```cuda
__global__ void embedding_f32_kernel(const int *idx, float *weight,
                                     float *output, int n, int emb_size) {
  int tx = threadIdx.x;
  int bx = blockIdx.x;  
  int offset = idx[bx] * emb_size;
  output[bx * emb_size + tx] = weight[offset + tx];
}
```

**原理：**
- 每个block处理一个sequence position
- 每个thread处理embedding向量的一个元素
- 线程配置：`grid(N), block(emb_size)`

**特点：**
- 最简单直接的实现
- 每个thread执行一次内存访问
- 适合小embedding size的场景

#### 2. embedding_f32x4_kernel (Float4向量化版本)
```cuda
__global__ void embedding_f32x4_kernel(const int *idx, float *weight,
                                       float *output, int n, int emb_size) {
  int tx = threadIdx.x * 4;
  int bx = blockIdx.x;
  int offset = idx[bx] * emb_size;
  output[bx * emb_size + tx] = weight[offset + tx];
  output[bx * emb_size + tx + 1] = weight[offset + tx + 1];
  output[bx * emb_size + tx + 2] = weight[offset + tx + 2];  
  output[bx * emb_size + tx + 3] = weight[offset + tx + 3];
}
```

**优化策略：**
- **向量化访问**：每个thread处理4个连续元素
- **减少线程数**：block size = `emb_size/4`
- **提高内存带宽利用率**：4个连续访问可能合并为更宽的内存事务

**优势：**
- 减少线程调度开销
- 提高内存访问效率
- 适合中等embedding size

#### 3. embedding_f32x4_pack_kernel (Float4打包版本)
```cuda
__global__ void embedding_f32x4_pack_kernel(const int *idx, float *weight,
                                            float *output, int n, int emb_size) {
  int tx = threadIdx.x;
  int bx = blockIdx.x;
  int offset = idx[bx] * emb_size;
  LDST128BITS(output[bx * emb_size + 4 * tx]) = 
      LDST128BITS(weight[offset + 4 * tx]);
}
```

**核心优化：**
- **128位打包访问**：使用`float4`类型进行128位对齐访问
- **单指令传输**：将4个float作为一个128位数据块传输
- **内存合并访问**：更好的内存访问模式

**技术细节：**
- `LDST128BITS`宏将地址转换为`float4*`指针
- 一次操作传输16字节(4×4字节)
- 要求数据128位对齐

#### 4. embedding_f16_kernel (Half精度基础版本)
```cuda
__global__ void embedding_f16_kernel(const int *idx, half *weight, half *output,
                                     int n, int emb_size) {
  // 与f32版本逻辑相同，使用half类型
}
```

**优势：**
- **内存带宽优化**：half精度占用内存减半
- **缓存效率提升**：更多数据可以存储在缓存中
- **适合现代GPU**：支持native half精度计算

#### 5. embedding_f16x8_kernel (Half精度8元素向量化)
```cuda
__global__ void embedding_f16x8_kernel(const int *idx, half *weight,
                                       half *output, int n, int emb_size) {
  int tx = threadIdx.x * 8;
  // 处理8个连续的half元素
}
```

**优化分析：**
- **更高向量化程度**：每线程处理8个half元素
- **内存带宽充分利用**：8×2字节=16字节，接近128位访问
- **线程数量优化**：block size = `emb_size/8`

#### 6. embedding_f16x8_pack_kernel (Half精度打包版本)
```cuda
__global__ void embedding_f16x8_pack_kernel(const int *idx, half *weight,
                                            half *output, int n, int emb_size) {
  LDST128BITS(output[bx * emb_size + 8 * tx]) = 
      LDST128BITS(weight[offset + 8 * tx]);
}
```

**最优化策略：**
- **128位对齐访问**：8个half (8×2=16字节) 正好128位
- **最佳内存效率**：单次操作传输最大数据量
- **减少指令数**：将8次单独访问合并为1次

### 性能对比分析

从测试结果可以观察到：

#### Float32版本性能特点：
- **f32 > f32x4 ≈ f32x4_pack**
- pack版本在f32下优势不明显，甚至略慢

**原因分析：**
1. **内存对齐影响**：float4可能存在对齐开销
2. **缓存行利用**：32位数据在某些情况下缓存利用率更高
3. **指令调度**：简单访问模式可能被GPU更好优化

#### Half精度版本性能特点：
- **f16x8_pack > f16x8 > f16**
- pack版本在f16下优势明显

**优势原因：**
1. **内存带宽充分利用**：8×half正好填满128位总线
2. **缓存效率**：half精度数据密度更高
3. **现代GPU优化**：针对half精度有专门优化

### 关键优化技术总结

#### 1. 向量化内存访问
- **原理**：将多个标量访问合并为向量访问
- **效果**：减少内存事务数量，提高带宽利用率
- **适用场景**：连续内存访问模式

#### 2. 内存访问对齐
- **128位对齐**：利用GPU 128位内存总线
- **合并访问**：相邻线程访问连续内存地址
- **减少bank conflicts**：避免共享内存冲突

#### 3. 数据类型优化
- **Half精度**：减少内存占用，提高缓存效率
- **适合模型推理**：在精度损失可接受的情况下

#### 4. 线程配置优化
- **减少线程数**：通过向量化减少线程调度开销
- **提高占用率**：优化block size以充分利用SM资源

### 应用建议

1. **小embedding size (<512)**：使用基础版本或小向量化
2. **中等embedding size (512-2048)**：使用4元素向量化
3. **大embedding size (>2048)**：使用pack版本
4. **精度允许情况**：优先选择half精度版本
5. **内存受限场景**：强烈推荐half精度pack版本
