# HardSwish

## 0x00 说明

包含以下内容：

- [X] hardswish_f32_kernel
- [X] hardswish_f32x4_kernel(float4向量化版本)
- [X] hardswish_f16_kernel(fp16版本)
- [X] hardswish_f16x2_kernel(fp16向量化版本)
- [X] hardswish_f16x8_kernel(fp16向量化版本)
- [X] hardswish_f16x8_pack_kernel(fp16向量化，pack版本)
- [X] PyTorch bindings


## 测试

```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada
python3 hardswish.py
```

输出:

```bash
-------------------------------------------------------------------------------------
                                        S=1024, K=1024
           out_f32: ['-0.31692463 ', '-0.13540865 '], time:0.00399518ms
         out_f32x4: ['-0.31692463 ', '-0.13540865 '], time:0.00348544ms
        out_f32_th: ['-0.31692463 ', '-0.13540865 '], time:0.00680089ms
-------------------------------------------------------------------------------------
           out_f16: ['-0.31713867 ', '-0.13537598 '], time:0.00405478ms
         out_f16x2: ['-0.31713867 ', '-0.13537598 '], time:0.00265884ms
         out_f16x8: ['-0.31713867 ', '-0.13537598 '], time:0.00252485ms
     out_f16x8pack: ['-0.31713867 ', '-0.13537598 '], time:0.00242925ms
        out_f16_th: ['-0.31689453 ', '-0.13537598 '], time:0.00608802ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=2048
           out_f32: ['0.02895264  ', '0.06114347  '], time:0.00553298ms
         out_f32x4: ['0.02895264  ', '0.06114347  '], time:0.00528932ms
        out_f32_th: ['0.02895264  ', '0.06114347  '], time:0.01048636ms
-------------------------------------------------------------------------------------
           out_f16: ['0.02894592  ', '0.06112671  '], time:0.00549722ms
         out_f16x2: ['0.02894592  ', '0.06112671  '], time:0.00471425ms
         out_f16x8: ['0.02894592  ', '0.06112671  '], time:0.00379252ms
     out_f16x8pack: ['0.02894592  ', '0.06112671  '], time:0.00358367ms
        out_f16_th: ['0.02894592  ', '0.06115723  '], time:0.00684929ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=4096
           out_f32: ['1.28615212  ', '0.17574076  '], time:0.00932550ms
         out_f32x4: ['1.28615212  ', '0.17574076  '], time:0.00886083ms
        out_f32_th: ['1.28615212  ', '0.17574076  '], time:0.01790905ms
-------------------------------------------------------------------------------------
           out_f16: ['1.28613281  ', '0.17578125  '], time:0.00924945ms
         out_f16x2: ['1.28613281  ', '0.17578125  '], time:0.00908136ms
         out_f16x8: ['1.28613281  ', '0.17578125  '], time:0.00614285ms
     out_f16x8pack: ['1.28613281  ', '0.17578125  '], time:0.00555348ms
        out_f16_th: ['1.28613281  ', '0.17578125  '], time:0.01063919ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=1024
           out_f32: ['2.739393    ', '0.04897798  '], time:0.00654101ms
         out_f32x4: ['2.739393    ', '0.04897798  '], time:0.00531363ms
        out_f32_th: ['2.739393    ', '0.04897798  '], time:0.01048684ms
-------------------------------------------------------------------------------------
           out_f16: ['2.73632812  ', '0.04898071  '], time:0.00676942ms
         out_f16x2: ['2.73632812  ', '0.04898071  '], time:0.00383520ms
         out_f16x8: ['2.73632812  ', '0.04898071  '], time:0.00384569ms
     out_f16x8pack: ['2.73632812  ', '0.04898071  '], time:0.00372910ms
        out_f16_th: ['2.73828125  ', '0.04898071  '], time:0.00684285ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=2048
           out_f32: ['0.00921244  ', '-0.36749047 '], time:0.00932741ms
         out_f32x4: ['0.00921244  ', '-0.36749047 '], time:0.00964785ms
        out_f32_th: ['0.00921244  ', '-0.36749047 '], time:0.01939940ms
-------------------------------------------------------------------------------------
           out_f16: ['0.00920868  ', '-0.36743164 '], time:0.00925016ms
         out_f16x2: ['0.00920868  ', '-0.36743164 '], time:0.00796676ms
         out_f16x8: ['0.00920868  ', '-0.36743164 '], time:0.00587964ms
     out_f16x8pack: ['0.00920868  ', '-0.36743164 '], time:0.00551319ms
        out_f16_th: ['0.00920868  ', '-0.36743164 '], time:0.01064467ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
           out_f32: ['1.23418164  ', '-0.35043269 '], time:0.01706600ms
         out_f32x4: ['1.23418164  ', '-0.35043269 '], time:0.01722789ms
        out_f32_th: ['1.23418164  ', '-0.35043269 '], time:0.09308505ms
-------------------------------------------------------------------------------------
           out_f16: ['1.23535156  ', '-0.35058594 '], time:0.01689029ms
         out_f16x2: ['1.23535156  ', '-0.35058594 '], time:0.01665306ms
         out_f16x8: ['1.23535156  ', '-0.35058594 '], time:0.01000905ms
     out_f16x8pack: ['1.23535156  ', '-0.35058594 '], time:0.00916457ms
        out_f16_th: ['1.234375    ', '-0.3503418  '], time:0.01818967ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=1024
           out_f32: ['1.13679588  ', '2.5862627   '], time:0.01175451ms
         out_f32x4: ['1.13679588  ', '2.5862627   '], time:0.00892878ms
        out_f32_th: ['1.13679588  ', '2.5862627   '], time:0.01798749ms
-------------------------------------------------------------------------------------
           out_f16: ['1.13671875  ', '2.5859375   '], time:0.01221919ms
         out_f16x2: ['1.13671875  ', '2.5859375   '], time:0.00619817ms
         out_f16x8: ['1.13671875  ', '2.5859375   '], time:0.00586224ms
     out_f16x8pack: ['1.13671875  ', '2.5859375   '], time:0.00551724ms
        out_f16_th: ['1.13671875  ', '2.5859375   '], time:0.01065254ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=2048
           out_f32: ['-0.11715776 ', '-0.03201698 '], time:0.01850605ms
         out_f32x4: ['-0.11715776 ', '-0.03201698 '], time:0.01827025ms
        out_f32_th: ['-0.11715776 ', '-0.03201698 '], time:0.09311175ms
-------------------------------------------------------------------------------------
           out_f16: ['-0.11712646 ', '-0.03201294 '], time:0.01689458ms
         out_f16x2: ['-0.11712646 ', '-0.03201294 '], time:0.01446867ms
         out_f16x8: ['-0.11712646 ', '-0.03201294 '], time:0.00979257ms
     out_f16x8pack: ['-0.11712646 ', '-0.03201294 '], time:0.00915074ms
        out_f16_th: ['-0.11712646 ', '-0.03204346 '], time:0.01819777ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=4096
           out_f32: ['-0.36491728 ', '1.09765351  '], time:0.14531279ms
         out_f32x4: ['-0.36491728 ', '1.09765351  '], time:0.14574075ms
        out_f32_th: ['-0.36491728 ', '1.09765351  '], time:0.29305243ms
-------------------------------------------------------------------------------------
           out_f16: ['-0.36499023 ', '1.09765625  '], time:0.03205943ms
         out_f16x2: ['-0.36499023 ', '1.09765625  '], time:0.03170896ms
         out_f16x8: ['-0.36499023 ', '1.09765625  '], time:0.01824880ms
     out_f16x8pack: ['-0.36499023 ', '1.09765625  '], time:0.01677060ms
        out_f16_th: ['-0.36499023 ', '1.09765625  '], time:0.09315753ms
-------------------------------------------------------------------------------------
```

## 0x01 Kernel原理分析

### HardSwish激活函数
HardSwish是一个高效的激活函数，由Google在MobileNetV3中提出。其数学表达式为：
```
HardSwish(x) = x * HardSigmoid(x)
HardSigmoid(x) = max(0, min(1, (x + 3) / 6))
```

简化为分段函数：
- 当 x ≥ 3 时，输出 x
- 当 x ≤ -3 时，输出 0
- 当 -3 < x < 3 时，输出 x * (x + 3) / 6

相比于Swish函数，HardSwish避免了指数运算，计算效率更高，同时保持了相似的激活特性。

## 0x02 各Kernel实现分析

### 1. hardswish_f32_kernel - 基础FP32实现
**实现原理：**
- 每个线程处理一个FP32元素
- 使用条件分支实现分段函数逻辑
- 直接的一对一映射：`tid -> 处理元素idx`

**优化特点：**
- 实现简单，易于理解和调试
- 适用于小规模数据或作为基准对比

**性能限制：**
- 分支发散可能影响warp执行效率
- 内存访问未优化，存在潜在的内存带宽浪费

### 2. hardswish_f32x4_kernel - FP32向量化优化
**实现原理：**
- 每个线程处理4个连续的FP32元素
- 使用`float4`向量类型进行内存合并访问
- 循环展开处理4个元素的计算

**优化技术：**
- **向量化内存访问：** 使用128位(16字节)的合并访问，提高内存带宽利用率
- **计算密度提升：** 每个线程的计算工作量增加4倍
- **减少线程数量：** 总线程数减少到N/4，降低线程管理开销

**性能提升原因：**
- 内存合并访问减少了内存事务数量
- 更高的算术强度(计算与内存访问比)
- 更好的GPU资源利用率

### 3. hardswish_f16_kernel - 基础FP16实现
**实现原理：**
- 使用half精度浮点数，减少内存占用
- 每个线程处理一个half元素
- 针对half类型的分支逻辑实现

**优化特点：**
- **内存优化：** FP16相比FP32节省50%内存带宽
- **缓存友好：** 更多数据可以放入缓存中
- **精度权衡：** 牺牲部分数值精度换取性能提升

### 4. hardswish_f16x2_kernel - FP16双元素向量化
**实现原理：**
- 每个线程处理2个连续的half元素
- 使用`half2`向量类型进行SIMD操作
- 32位对齐的内存访问

**优化技术：**
- **SIMD并行：** 利用GPU的SIMD指令集
- **内存对齐：** 32位对齐访问提高内存效率
- **向量化计算：** 减少指令数量

### 5. hardswish_f16x8_kernel - FP16八元素向量化
**实现原理：**
- 每个线程处理8个连续的half元素
- 使用4个`half2`向量，分别处理
- 边界检查确保内存访问安全

**优化技术：**
- **高度向量化：** 显著提高计算密度
- **内存合并优化：** 128位内存访问模式
- **循环展开：** 手动展开循环减少控制开销
- **边界保护：** 细粒度的边界检查避免越界访问

**性能特点：**
- 最高的计算吞吐量
- 对内存带宽的充分利用
- 适合大规模数据处理

### 6. hardswish_f16x8_pack_kernel - FP16打包优化版本
**实现原理：**
- 使用128位加载/存储指令(`LDST128BITS`)
- 将8个half元素打包到本地数组中
- 使用pragma unroll指令优化循环

**核心优化技术：**
- **128位原子操作：** 单指令完成16字节的内存传输
- **寄存器优化：** 使用本地数组优化寄存器使用
- **编译器指导：** `#pragma unroll`指导编译器完全展开循环
- **内存访问模式优化：** 最大化内存带宽利用率

**性能优势：**
- 最优的内存访问效率
- 最少的内存事务数量
- 最高的计算与内存访问比
- 测试结果显示性能最佳

## 0x03 优化方法总结

### 内存访问优化
1. **向量化加载/存储：** 使用float4、half2等向量类型
2. **内存合并：** 确保连续内存访问模式
3. **128位访问：** 使用LDST128BITS宏实现最优内存带宽

### 计算优化
1. **SIMD并行：** 利用GPU的向量指令集
2. **循环展开：** 减少控制流开销
3. **寄存器优化：** 合理使用寄存器资源

### 线程组织优化
1. **工作负载平衡：** 每个线程处理多个元素
2. **Warp效率：** 减少分支发散影响
3. **占用率优化：** 平衡线程数和资源使用

### 精度与性能权衡
1. **FP16优化：** 在可接受精度损失下获得显著性能提升
2. **分段函数优化：** 避免复杂数学运算
3. **条件分支优化：** 尽可能减少分支发散

## 0x04 性能对比分析

从测试结果可以看出：
- **FP16版本普遍优于FP32：** 内存带宽优势明显
- **向量化程度越高性能越好：** f16x8_pack > f16x8 > f16x2 > f16
- **PyTorch原生实现性能较差：** 说明手工优化的必要性
- **大数据量下优势更明显：** 向量化在大规模计算中效果显著

最优版本`hardswish_f16x8_pack`在各种数据规模下都表现出最佳性能，体现了综合优化技术的有效性。
