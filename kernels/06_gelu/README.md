# GELU

## 0x00 说明

包含以下内容：

- [X] gelu_f32_kernel
- [X] gelu_f32x4_kernel(float4向量化版本)
- [X] gelu_f16_kernel
- [X] gelu_f16x2_kernel(half2向量化)
- [X] gelu_f16x8_kernel(unpack版本)
- [X] gelu_f16x8_pack_kernel(pack版本)
- [X] PyTorch bindings


## 测试

对于半精度(half)的GELU操作，由于CUDA的半精度计算中并不包含tanh操作，因此需要使用hexp来替代对应的操作，因此会引入较大的误差。（或许可以考虑从汇编上解决这个问题）;而torch是通过转化数据类型完成的。想要测试很简单，修改一下cu中f16里面的代码做一下强制类型转换即可：

```c++
y[idx]  = HALF_GELU_OPS(__half2float(v)); // line 96
reg_y.x = HALF_GELU_OPS(__half2float(reg_x.x)); // line 109 , line 110
reg_y.y = HALF_GELU_OPS(__half2float(reg_x.y));
```
测试结果如下（由于不是所有数据都会掉误差所以取了会有误差的情况，可见修改后out_f16和out_f16x2的结果和torch相同了）：
```bash
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
           out_f32: [-0.08196318, -0.1613517], time:0.13425708ms
         out_f32x4: [-0.08196318, -0.1613517], time:0.14128804ms
        out_f32_th: [-0.08196313, -0.1613517], time:0.08195782ms
-------------------------------------------------------------------------------------
           out_f16: [-0.08197021, -0.16137695], time:0.12120271ms
         out_f16x2: [-0.08197021, -0.16137695], time:0.12122369ms
         out_f16x8: [-0.08251953, -0.16137695], time:0.04196978ms
     out_f16x8pack: [-0.08251953, -0.16137695], time:0.04215288ms
        out_f16_th: [-0.08197021, -0.16137695], time:0.04287958ms
-------------------------------------------------------------------------------------
```
相关参考：
- [pytorch-c10-BFloat16.h](https://github.com/pytorch/pytorch/blob/main/c10/util/BFloat16.h)
- [math ptx](https://github.com/pavanky/math_ptx)

此外仿照torch实现了在float下tanh和none两种近似下的GELU函数，可以在gelu.cu的宏中进行修改实现不同的版本的编译。

```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada
python3 gelu.py
```

输出（不做类型转换导致half误差）:

```bash
-------------------------------------------------------------------------------------
                                        S=1024, K=1024
           out_f32: [-0.13358943, -0.06881647], time:0.01621890ms
         out_f32x4: [-0.13358943, -0.06881647], time:0.01278400ms
        out_f32_th: [-0.13358943, -0.06881647], time:0.00897789ms
-------------------------------------------------------------------------------------
           out_f16: [-0.13378906, -0.06884766], time:0.00663781ms
         out_f16x2: [-0.13378906, -0.06884766], time:0.00366306ms
         out_f16x8: [-0.13378906, -0.06884766], time:0.00343323ms
     out_f16x8pack: [-0.13378906, -0.06884766], time:0.00331473ms
        out_f16_th: [-0.13354492, -0.06884766], time:0.00907278ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=2048
           out_f32: [1.38783729, -0.06707606], time:0.02223682ms
         out_f32x4: [1.38783729, -0.06707606], time:0.02367806ms
        out_f32_th: [1.38783729, -0.06707606], time:0.00959325ms
-------------------------------------------------------------------------------------
           out_f16: [1.38769531, -0.06713867], time:0.00834370ms
         out_f16x2: [1.38769531, -0.06713867], time:0.00784707ms
         out_f16x8: [1.38769531, -0.06713867], time:0.00499964ms
     out_f16x8pack: [1.38769531, -0.06713867], time:0.00461078ms
        out_f16_th: [1.38769531, -0.06707764], time:0.00895357ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=4096
           out_f32: [0.47386399, 0.05760021], time:0.04273629ms
         out_f32x4: [0.47386399, 0.05760021], time:0.05011940ms
        out_f32_th: [0.47386405, 0.05760022], time:0.00933146ms
-------------------------------------------------------------------------------------
           out_f16: [0.47387695, 0.05761719], time:0.01495123ms
         out_f16x2: [0.47387695, 0.05761719], time:0.01039743ms
         out_f16x8: [0.47387695, 0.05761719], time:0.00936055ms
     out_f16x8pack: [0.47387695, 0.05761719], time:0.00845838ms
        out_f16_th: [0.47387695, 0.05758667], time:0.00918818ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=1024
           out_f32: [1.3562144, 0.40408486], time:0.03009892ms
         out_f32x4: [1.3562144, 0.40408486], time:0.02289677ms
        out_f32_th: [1.3562144, 0.40408486], time:0.00921512ms
-------------------------------------------------------------------------------------
           out_f16: [1.35644531, 0.40405273], time:0.01173806ms
         out_f16x2: [1.35644531, 0.40405273], time:0.00565076ms
         out_f16x8: [1.35644531, 0.40405273], time:0.00502610ms
     out_f16x8pack: [1.35644531, 0.40405273], time:0.00457048ms
        out_f16_th: [1.35644531, 0.40429688], time:0.00904894ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=2048
           out_f32: [-0.16498716, -0.15077244], time:0.04273534ms
         out_f32x4: [-0.16498716, -0.15077244], time:0.04386163ms
        out_f32_th: [-0.16498716, -0.15077244], time:0.00913596ms
-------------------------------------------------------------------------------------
           out_f16: [-0.16516113, -0.15075684], time:0.01495862ms
         out_f16x2: [-0.16516113, -0.15075684], time:0.01407337ms
         out_f16x8: [-0.16516113, -0.15075684], time:0.00796247ms
     out_f16x8pack: [-0.16516113, -0.15075684], time:0.00734925ms
        out_f16_th: [-0.16503906, -0.15075684], time:0.00917435ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
           out_f32: [-0.03888749, 0.32139146], time:0.08363676ms
         out_f32x4: [-0.03888749, 0.32139146], time:0.09505510ms
        out_f32_th: [-0.03888749, 0.32139146], time:0.04022837ms
-------------------------------------------------------------------------------------
           out_f16: [-0.03887939, 0.3215332], time:0.02813959ms
         out_f16x2: [-0.03887939, 0.3215332], time:0.01906514ms
         out_f16x8: [-0.03887939, 0.3215332], time:0.01664281ms
     out_f16x8pack: [-0.03887939, 0.3215332], time:0.01474833ms
        out_f16_th: [-0.03887939, 0.32128906], time:0.01357365ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=1024
           out_f32: [-0.13875209, 1.08477271], time:0.05790567ms
         out_f32x4: [-0.13875209, 1.08477271], time:0.04317236ms
        out_f32_th: [-0.13875209, 1.08477271], time:0.00910425ms
-------------------------------------------------------------------------------------
           out_f16: [-0.13903809, 1.08496094], time:0.02198315ms
         out_f16x2: [-0.13903809, 1.08496094], time:0.00964355ms
         out_f16x8: [-0.13903809, 1.08496094], time:0.00780869ms
     out_f16x8pack: [-0.13903809, 1.08496094], time:0.00729132ms
        out_f16_th: [-0.13879395, 1.08496094], time:0.00926042ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=2048
           out_f32: [0.82045084, -0.0894338], time:0.08363843ms
         out_f32x4: [0.82045084, -0.0894338], time:0.08431888ms
        out_f32_th: [0.82045084, -0.0894338], time:0.03837347ms
-------------------------------------------------------------------------------------
           out_f16: [0.8203125, -0.08947754], time:0.02813506ms
         out_f16x2: [0.8203125, -0.08947754], time:0.02643061ms
         out_f16x8: [0.8203125, -0.08947754], time:0.01383305ms
     out_f16x8pack: [0.8203125, -0.08947754], time:0.01273918ms
        out_f16_th: [0.82080078, -0.0894165], time:0.01357722ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=4096
           out_f32: [-0.06997654, -0.16092129], time:0.19113564ms
         out_f32x4: [-0.06997654, -0.16092129], time:0.20371628ms
        out_f32_th: [-0.06997654, -0.16092129], time:0.20496607ms
-------------------------------------------------------------------------------------
           out_f16: [-0.07012939, -0.16113281], time:0.05451322ms
         out_f16x2: [-0.07012939, -0.16113281], time:0.03633785ms
         out_f16x8: [-0.07012939, -0.16113281], time:0.03115463ms
     out_f16x8pack: [-0.07012939, -0.16113281], time:0.02735877ms
        out_f16_th: [-0.07000732, -0.16088867], time:0.03889561ms
-------------------------------------------------------------------------------------
```

## Kernel 原理与优化分析

### GELU函数原理

GELU (Gaussian Error Linear Unit) 是一个重要的激活函数，数学公式为：

$$GELU(x) = 0.5x(1 + tanh(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)))$$

该函数结合了随机正则化和确定性函数的特性，在Transformer等模型中广泛应用。

### Kernel实现分析

#### 1. gelu_f32_kernel (基础FP32版本)
```cuda
__global__ void gelu_f32_kernel(float *x, float *y, int N)
```
**工作原理**：
- 每个线程处理一个float元素
- 使用标准的GELU tanh近似公式
- 线程映射：`idx = blockIdx.x * blockDim.x + threadIdx.x`

**优化特点**：
- 数值稳定性：使用`fminf(fmaxf(x[idx], MIN_EXP_F32), MAX_EXP_F32)`防止指数溢出
- 简单直接的一对一映射，适合小规模数据处理

#### 2. gelu_f32x4_kernel (FP32向量化版本)
```cuda
__global__ void gelu_f32x4_kernel(float *x, float *y, int N)
```
**工作原理**：
- 每个线程处理4个连续的float元素
- 使用`float4`向量化数据类型

**优化方法**：
- **内存带宽优化**：`float4`一次加载128位数据，减少内存事务数量
- **指令级并行**：4个元素同时计算，提高ALU利用率
- **合并访问**：连续的4个float元素保证GPU内存合并访问模式
- **吞吐量提升**：理论上可获得接近4倍的吞吐量提升

#### 3. gelu_f16_kernel (基础FP16版本)
```cuda
__global__ void gelu_f16_kernel(half *x, half *y, int N)
```
**工作原理**：
- 使用half精度进行GELU计算
- 自定义`gelu_tanh_approximate(half x)`函数

**优化特点**：
- **内存效率**：half精度减少50%内存占用
- **精度权衡**：牺牲部分精度换取性能和内存效率
- **自定义实现**：由于CUDA缺少half精度tanh函数，使用`hexp`实现近似计算

**注意事项**：
- 引入数值误差，需要权衡精度与性能
- 可通过类型转换获得更高精度：`__half2float(v)`

#### 4. gelu_f16x2_kernel (FP16向量化版本)
```cuda
__global__ void gelu_f16x2_kernel(half *x, half *y, int N)
```
**工作原理**：
- 每个线程处理2个half元素
- 使用`half2`向量化类型

**优化方法**：
- **SIMD利用**：利用GPU的向量处理单元同时计算2个half
- **内存效率**：`half2`优化内存访问模式
- **寄存器优化**：减少寄存器使用，提高occupancy

#### 5. gelu_f16x8_kernel (FP16 unpack版本)
```cuda
__global__ void gelu_f16x8_kernel(half *x, half *y, int N)
```
**工作原理**：
- 每个线程处理8个half元素
- 使用4个`half2`分别加载和处理数据
- 独立的边界检查和存储操作

**优化方法**：
- **高度向量化**：8元素并行处理，最大化ALU利用率
- **灵活性**：分离的加载/存储操作，便于边界处理
- **细粒度控制**：每个`half2`独立操作，避免不必要的计算

#### 6. gelu_f16x8_pack_kernel (FP16 pack优化版本)
```cuda
__global__ void gelu_f16x8_pack_kernel(half *x, half *y, int N)
```
**工作原理**：
- 使用本地数组`pack_x[8], pack_y[8]`作为缓存
- 通过`LDST128BITS`宏实现128位对齐的批量加载/存储
- 循环展开处理8个元素

**优化方法**：
- **内存合并**：128位对齐访问，最大化内存带宽利用率
- **缓存优化**：本地数组减少全局内存访问次数
- **批量处理**：一次性处理8个元素，减少内存事务开销
- **寄存器管理**：使用本地内存而非寄存器，适应不同GPU架构

### 性能对比与选择建议

**内存带宽利用率**：
```
f16x8_pack > f16x8 > f32x4 > f16x2 > f16 > f32
```

**计算精度**：
```
f32/f32x4 > f16(with conversion) > f16(native)
```

**适用场景**：
- **高精度需求**：使用f32版本
- **平衡性能与精度**：使用f16x8_pack
- **极致性能**：根据数据规模选择合适的向量化版本
- **内存受限**：优先选择pack版本

### 关键优化策略

#### 1. 内存访问优化
- **向量化加载**：使用`float4`、`half2`等向量类型
- **合并访问**：保证连续内存访问模式，避免内存带宽浪费
- **对齐访问**：128位对齐访问优化内存控制器效率

#### 2. 计算优化
- **指令级并行**：多元素并行计算，充分利用ALU资源
- **数值稳定性**：通过范围限制防止数值溢出
- **精度权衡**：在半精度和单精度间做出合理选择

#### 3. 架构适配
- **不同向量宽度**：从1到8元素的不同并行度适应不同workload
- **内存层次优化**：合理利用寄存器、共享内存和全局内存
- **占用率优化**：平衡寄存器使用和线程块大小
